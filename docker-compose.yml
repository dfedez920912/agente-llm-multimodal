# docker-compose.yml
version: '3.8'
services:
  llm-agent:
    container_name: ${CONTAINER_NAME} # <-- Nombre del contenedor
    build:
      context: .
      # Pasar el token como argumento de build
      args:
        - HF_TOKEN=${HF_TOKEN}
    restart: always
    environment:
      - API_KEY=${API_KEY}
      - LLM_MODEL_NAME=${LLM_MODEL_NAME}
      - VL_LLM_MODEL_NAME=${VL_LLM_MODEL_NAME}
      - ASR_MODEL_NAME=${ASR_MODEL_NAME}
      - TTS_MODEL_NAME=${TTS_MODEL_NAME} 
      - MODEL_LOADING_STRATEGY=${MODEL_LOADING_STRATEGY}
      # Pasar el token también como variable de entorno en tiempo de ejecución
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all  # <-- Mover esta línea aquí
    ports:
      - "8000:8000"
    volumes:
      - model-cache:/root/.cache/huggingface
    runtime: nvidia  # <-- Correcto para Windows/WSL2

volumes:
  model-cache:
    driver: local

networks:
  default:
    name: ${CONTAINER_NAME}  # <-- Nombre de la red predeterminada
    # driver: local # <-- 'local' no es un driver válido aquí. Se comenta.