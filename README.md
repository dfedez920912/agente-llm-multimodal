# Servidor LLM Multimodal Local

## üöÄ Visi√≥n General

Este proyecto implementa un servidor LLM (Large Language Model) multimodal local, dise√±ado para procesar mensajes de texto, im√°genes y audio desde conversaciones de WhatsApp a trav√©s de **EvolutionAPI**. El servidor emula la API de **OpenAI** para permitir una integraci√≥n fluida con **n8n** sin necesidad de modificar los flujos de trabajo existentes. El objetivo es ofrecer una soluci√≥n de IA privada y de bajo coste, ideal para el desarrollo y las pruebas antes de pasar a servicios de pago.

---

## ‚ú® Caracter√≠sticas Principales

-   **Compatibilidad con n8n**: Emula la API de OpenAI, lo que permite el uso de nodos nativos de n8n.
-   **Procesamiento Multimodal**: Maneja im√°genes (Base64), audio (Base64) y texto.
-   **S√≠ntesis de Voz (TTS)**: *Opcionalmente*, puede responder con voz, generando audio en base64.
-   **Eficiencia de Hardware**: Optimizado para funcionar en una **RTX 3060 (12GB)** mediante la cuantizaci√≥n de modelos.
-   **Arquitectura Escalable**: Configurado con `docker-compose` para una transici√≥n sin problemas a hardware m√°s potente (ej. 4x RTX 4090) con solo cambiar un archivo de entorno.
-   **Autenticaci√≥n Segura**: Utiliza una API Key para proteger el endpoint.

---

## üõ†Ô∏è Despliegue Paso a Paso sistemas Windows

### **Variante A: Compilaci√≥n Local (`docker-compose up --build`)**

1.  **Clonar el repositorio:**
    ```bash
    git clone https://github.com/dfedez920912/agente-llm-multimodal.git
    cd agente-llm-multimodal
    ```
2.  **Configurar las variables de entorno:**
    Copia el archivo de plantilla `.env.base` a `.env` y ed√≠talo con tus datos.
    ```bash
    cp .env.base .env
    ```
    Ahora, edita el nuevo archivo `.env` con tu clave de API y otros par√°metros si lo deseas.
3.  **Iniciar el servicio con Docker Compose (compilando localmente):**
    Aseg√∫rate de tener Docker Desktop y los drivers de NVIDIA configurados. Luego, ejecuta:
    ```bash
    docker-compose up --build -d
    ```
    El servicio estar√° disponible en `http://localhost:8000`.

### **Variante B: Descarga de Imagen Precompilada desde GitHub (`docker-compose up`)**

1.  **Clonar el repositorio (o descargar los ficheros `docker-compose.yml` y `.env`):**
    ```bash
    git clone https://github.com/dfedez920912/agente-llm-multimodal.git
    cd agente-llm-multimodal
    ```
2.  **Configurar las variables de entorno:**
    Copia el archivo de plantilla `.env.base` a `.env` y ed√≠talo con tus datos.
    ```bash
    cp .env.base .env
    ```
    Ahora, edita el nuevo archivo `.env` con tu clave de API y otros par√°metros si lo deseas.
3.  **Modificar `docker-compose.yml` para usar la imagen precompilada:**
    Edita el archivo `docker-compose.yml` y **reemplaza** la l√≠nea `build: .` por `image: ghcr.io/dfedez920912/agente-llm-multimodal:main` (o la etiqueta deseada). Aseg√∫rate de que la secci√≥n `deploy` est√© reemplazada por `runtime: nvidia` (como se describe en las mejoras).
    ```yaml
    # docker-compose.yml (fragmento)
    services:
      llm-agent:
        image: ghcr.io/dfedez920912/agente-llm-multimodal:main # <-- Usar imagen desde GHCR
        # build: . # <-- Comentar o eliminar esta l√≠nea
        restart: always
        environment:
          - API_KEY=${API_KEY}
          - LLM_MODEL_NAME=${LLM_MODEL_NAME}
          - VL_LLM_MODEL_NAME=${VL_LLM_MODEL_NAME}
          - ASR_MODEL_NAME=${ASR_MODEL_NAME}
          - TTS_MODEL_NAME=${TTS_MODEL_NAME} # <-- Aseg√∫rate de tener esta si TTS est√° integrado
          - MODEL_LOADING_STRATEGY=${MODEL_LOADING_STRATEGY}
          - NVIDIA_VISIBLE_DEVICES=all # <-- Aseg√∫rate de tener esta l√≠nea
        ports:
          - "8000:8000"
        volumes:
          - model-cache:/root/.cache/huggingface
        runtime: nvidia # <-- Crucial para Windows/WSL2
    ```
4.  **Iniciar el servicio con Docker Compose (descargando imagen):**
    Aseg√∫rate de tener Docker Desktop y los drivers de NVIDIA configurados. Luego, ejecuta:
    ```bash
    docker-compose up -d
    ```
    Docker Compose descargar√° la imagen desde GitHub Container Registry y la ejecutar√°. El servicio estar√° disponible en `http://localhost:8000`.

---

## üõ†Ô∏è Despliegue Paso a Paso sistemas Linux

### **Variante A: Compilaci√≥n Local (`docker-compose up --build`)**

1.   **Actualizar el Sistema:**
     Antes de instalar nada, es fundamental que actualices la lista de paquetes y las versiones de tu sistema:
    ```bash
     sudo apt update
     sudo apt upgrade -y
    ```

2.  **Instalar Docker y Docker Compose:**
    Sigue las instrucciones oficiales de Docker para instalar `docker-ce`, `docker-ce-cli` y `docker-compose-plugin` en Ubuntu.

3.  **Instalar los drivers de NVIDIA:**
    *   **Identificar el Driver Recomendado:**
        Usa el siguiente comando para que Ubuntu te muestre los drivers disponibles y te sugiera cu√°l es el mejor para tu GPU:
        ```bash
        ubuntu-drivers devices
        ```
        La salida te mostrar√° una lista de drivers, con una recomendaci√≥n entre par√©ntesis, como (recommended).
    *   **Instalar el Driver Recomendado:**
        Ahora, usa el comando autoinstall para que Ubuntu instale autom√°ticamente el driver recomendado y las dependencias necesarias:
        ```bash
        sudo ubuntu-drivers autoinstall
        ```
        Este proceso se encarga de todo, incluso de instalar el kit de herramientas CUDA si es necesario para el driver.
    *   **Reiniciar el Servidor:**
        Para que los cambios surtan efecto y el kernel cargue los nuevos controladores, debes reiniciar el servidor.
        ```bash
        sudo reboot
        ```
    *   **Verificar la Instalaci√≥n:**
        Una vez que el servidor se haya reiniciado, vuelve a iniciar sesi√≥n y ejecuta el comando nvidia-smi.
        ```bash
        nvidia-smi
        ```
        Si la instalaci√≥n fue exitosa, ver√°s una tabla con la informaci√≥n de tu GPU, el uso de VRAM y la versi√≥n del driver.  Si el comando no funciona, algo sali√≥ mal durante la instalaci√≥n y tendr√°s que revisar los logs.

4.  **Clonar el repositorio:**
    ```bash
    git clone https://github.com/dfedez920912/agente-llm-multimodal.git
    cd agente-llm-multimodal
    ```
5.  **Configurar las variables de entorno:**
    Copia el archivo de plantilla `.env.base` a `.env` y ed√≠talo con tus datos.
    ```bash
    cp .env.base .env
    ```
    Ahora, edita el nuevo archivo `.env` con tu clave de API y otros par√°metros si lo deseas.
6.  **Iniciar el servicio con Docker Compose (compilando localmente):**
    ```bash
    docker-compose up --build -d
    ```
    El servicio estar√° disponible en `http://localhost:8000`.

### **Variante B: Descarga de Imagen Precompilada desde GitHub (`docker-compose up`)**

Sigue los pasos 1 a 3 de la Variante A (Actualizar sistema, Instalar Docker, Instalar drivers NVIDIA).

4.  **Clonar el repositorio (o descargar los ficheros `docker-compose.yml` y `.env`):**
    ```bash
    git clone https://github.com/dfedez920912/agente-llm-multimodal.git
    cd agente-llm-multimodal
    ```
5.  **Configurar las variables de entorno:**
    Copia el archivo de plantilla `.env.base` a `.env` y ed√≠talo con tus datos.
    ```bash
    cp .env.base .env
    ```
    Ahora, edita el nuevo archivo `.env` con tu clave de API y otros par√°metros si lo deseas.
6.  **Modificar `docker-compose.yml` para usar la imagen precompilada:**
    Edita el archivo `docker-compose.yml` y **reemplaza** la l√≠nea `build: .` por `image: ghcr.io/dfedez920912/agente-llm-multimodal:main` (o la etiqueta deseada). Aseg√∫rate de que la secci√≥n `deploy` est√© reemplazada por `runtime: nvidia`.
    ```yaml
    # docker-compose.yml (fragmento)
    services:
      llm-agent:
        image: ghcr.io/dfedez920912/agente-llm-multimodal:main # <-- Usar imagen desde GHCR
        # build: . # <-- Comentar o eliminar esta l√≠nea
        restart: always
        environment:
          - API_KEY=${API_KEY}
          - LLM_MODEL_NAME=${LLM_MODEL_NAME}
          - VL_LLM_MODEL_NAME=${VL_LLM_MODEL_NAME}
          - ASR_MODEL_NAME=${ASR_MODEL_NAME}
          - TTS_MODEL_NAME=${TTS_MODEL_NAME} # <-- Aseg√∫rate de tener esta si TTS est√° integrado
          - MODEL_LOADING_STRATEGY=${MODEL_LOADING_STRATEGY}
          - NVIDIA_VISIBLE_DEVICES=all # <-- Aseg√∫rate de tener esta l√≠nea
        ports:
          - "8000:8000"
        volumes:
          - model-cache:/root/.cache/huggingface
        runtime: nvidia # <-- Crucial para Linux con GPU
    ```
7.  **Iniciar el servicio con Docker Compose (descargando imagen):**
    ```bash
    docker-compose up -d
    ```
    Docker Compose descargar√° la imagen desde GitHub Container Registry y la ejecutar√°. El servicio estar√° disponible en `http://localhost:8000`.

---

## üß© Explicaci√≥n de los Cambios Realizados

### **1. Mejora de `app.py`**

*   **Uso correcto de `AutoTokenizer`:** Se cambi√≥ `AutoProcessor.from_pretrained` por `AutoTokenizer.from_pretrained` para el modelo de lenguaje (LLM), ya que `AutoProcessor` no es el adecuado para el flujo de texto puro con Llama 3.
*   **Aplicaci√≥n de `chat_template`:** Se a√±adi√≥ el uso de `llm_tokenizer.apply_chat_template()` para aplicar el formato oficial de chat de Llama 3, mejorando la calidad y coherencia de las respuestas de texto.
*   **Manejo seguro de archivos temporales:** Se reemplaz√≥ el uso de `temp_audio.wav` por `tempfile.NamedTemporaryFile` para manejar de forma segura los archivos de audio decodificados, evitando conflictos en concurrencia.
*   **Manejo de errores y logs:** Se a√±adieron bloques `try/except` y mensajes de log (`print`) claros (`‚úî` / `‚ùå`) para la carga de modelos, facilitando la depuraci√≥n.
*   **Integraci√≥n de S√≠ntesis de Voz (TTS):** Se a√±adieron importaciones, variables globales, funciones de carga din√°mica y l√≥gica en el endpoint `/v1/chat/completions` para generar audio a partir de la respuesta de texto si se solicita (mediante un campo opcional en la solicitud).
*   **Actualizaci√≥n de Pydantic:** Se a√±adi√≥ el campo `audio_content` opcional al modelo `Message` para devolver el audio generado en la respuesta.
*   **Soporte para `image_url` (OpenAI):** Se integr√≥ el manejo del formato est√°ndar de OpenAI para im√°genes en URL, junto con el formato personalizado `image_base64`.

### **2. Correcci√≥n de `docker-compose.yml`**

*   **Compatibilidad con Windows/WSL2 y Linux:** Se reemplaz√≥ la secci√≥n `deploy: ...` (dise√±ada para Docker Swarm) por `runtime: nvidia`. Esta es la forma correcta de asignar la GPU a un contenedor en Docker Compose standalone, lo que es esencial para que el contenedor funcione correctamente en Windows con WSL2 o en Linux con drivers NVIDIA instalados.
*   **Uso de `NVIDIA_VISIBLE_DEVICES`:** Se a√±adi√≥ `NVIDIA_VISIBLE_DEVICES=all` como variable de entorno para asegurar que el contenedor vea todas las GPUs NVIDIA disponibles.

### **3. Integraci√≥n de TTS en `.env`**

*   **Nueva variable `TTS_MODEL_NAME`:** Se a√±adi√≥ `TTS_MODEL_NAME` al archivo `.env` para permitir configurar el modelo de s√≠ntesis de voz desde las variables de entorno.

### **4. Actualizaci√≥n de `requirements.txt`**

*   **Adici√≥n de `TTS`:** Se a√±adi√≥ la biblioteca `TTS==0.22.0` para permitir la funcionalidad de s√≠ntesis de voz.

### **5. Workflow de GitHub Actions (`docker-build.yml`)**

*   **Automatizaci√≥n del Build y Push:** Se configur√≥ un workflow para que, con cada push a la rama `main`, se construya la imagen Docker y se suba al GitHub Container Registry (GHCR) con etiquetas √∫tiles (`latest`, `sha-commit`, `timestamp`).

---

## ü§ù Contribuidores

¬°Gracias a estos incre√≠bles colaboradores! 

<a href="https://github.com/dfedez920912  "><img src="https://github.com/tu-usuario-1.png  " width="50" alt="Daniel Fernandez Sotolongo"></a>
---

## ‚ù§Ô∏è Apoyar el Proyecto

Si este proyecto te ha sido √∫til, por favor considera apoyar su desarrollo. ¬°Cada donaci√≥n ayuda a cubrir los costos y a seguir mejorando!

* **PayPal**: [daniel920912](https://paypal.me/dfedez?locale.x=es_XC&country.x=US)
* **EnZona (Cuba)**:
    <br>
    <img src="assets/enzona_qr.png" alt="C√≥digo QR EnZona para donar" width="200" height="200"> <!-- Ajusta el nombre del archivo y el tama√±o -->
    <br>
* **Transferm√≥vil (Cuba)**: 
    <br>
    <img src="assets/transfermovil_qr.jpg" alt="C√≥digo QR EnZona para donar" width="200" height="200"> <!-- Ajusta el nombre del archivo y el tama√±o -->
    <br>
* **QvaPay**: [dfedez920912](https://qvapay.com/payme/daniel920912)